{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import feature_selection\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import completeness_score, homogeneity_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Automatic Document Clustering [Dataset: newsgroups5.zip]\n",
    "For this problem you will use a different subset of the 20 Newsgroup data set that you used in Assignment 2  (see the description of the full dataset). The subset for this assignment includes 2,500 documents (newsgroup posts), each belonging to one of 5 categories windows (0), crypt (1), christian (2), hockey (3), forsale (4). The documents are represented by 9328 terms (stems). The dictionary (vocabulary) for the data set is given in the file \"terms.txt\" and the full term-by-document matrix is given in \"matrix.txt\" (comma separated values). The actual category labels for the documents are provided in the file \"classes.txt\". Your goal in this assignment is to perform clustering on the documents and compare the clusters to the actual categories.\n",
    "\n",
    "Your tasks in this problem are the following [Note: for the clustering part of this assignment you should use the kMeans module form Ch. 10 of MLA (use the version provided here as it includes some corrections to the book version). You may also use Pandas and other modules from scikit-learn that you may need for preprocessing or evaluation.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a Create your own distance function that, instead of using Euclidean distance, uses Cosine similarity. This is the distance function you will use to pass to the kMeans function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cos_sim(obj1, obj2):\n",
    "    obj1_norm = np.linalg.norm(obj1)\n",
    "    obj2_norm = np.linalg.norm(obj2)\n",
    "    sim = np.dot(obj2,obj1)/(obj2_norm * obj1_norm)\n",
    "    dists = 1- sim\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b Load the data set [Note: the data matrix provided has terms as rows and documents as columns. Since you will be clustering documents, you'll need to take the transpose of this matrix so that your main data matrix is a document x term matrix. In Numpy, you may use the \".T\" operation to obtain the transpose.] Then, split the data set (the document x term matrix) and set aside 20% for later use (see below). Use the 80% segment for clustering in the next part. The 20% portion must be a random subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Import Training Data, Testing Data and Labels\n",
    "term_doc = pd.read_table('matrix.txt',sep=',', header = None)\n",
    "terms = pd.read_table('terms.txt', header = None)\n",
    "labels = pd.DataFrame(np.genfromtxt(\"classes.txt\", delimiter=' ',dtype=int,skip_header=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Convert term-dco to doc-term \n",
    "doc_term = term_doc.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9318</th>\n",
       "      <th>9319</th>\n",
       "      <th>9320</th>\n",
       "      <th>9321</th>\n",
       "      <th>9322</th>\n",
       "      <th>9323</th>\n",
       "      <th>9324</th>\n",
       "      <th>9325</th>\n",
       "      <th>9326</th>\n",
       "      <th>9327</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 9328 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...   9318  \\\n",
       "0     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "1     0     0     0     0     0     0     0     0     0     1  ...      0   \n",
       "2     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "3     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "4     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "\n",
       "   9319  9320  9321  9322  9323  9324  9325  9326  9327  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 9328 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1\n",
       "0  0\n",
       "1  1\n",
       "2  1\n",
       "3  1\n",
       "4  2"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1\n",
       "0  0\n",
       "1  1\n",
       "2  1\n",
       "3  1\n",
       "4  2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Remove the index\n",
    "labels.drop(0, axis=1, inplace=True)\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 80% - 20% split\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc_term, labels, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.c As in the case of Assignment 2, transform the term-frequencies to tfxidf values. Be sure to maintain DF values for each of the terms in the dictionary. [Note: if you run into problems due to limited computational resources, you may prune the data by removing all terms with low DF values, e.g., terms that appear in less than 10 documents. Be sure to maintain the correspondence between the dictionary terms and the matrix rows.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_T = X_train.T\n",
    "X_test_T = X_test.T\n",
    "\n",
    "### Store some values\n",
    "NTerm = X_train.shape[1]\n",
    "NDoc_train = X_train.shape[0]\n",
    "NDoc_test = X_test.shape[0]\n",
    "\n",
    "### Document Frequency\n",
    "DF = np.array([(X_train_T!=0).sum(1)]).T\n",
    "\n",
    "### IDF\n",
    "train_Mat = np.ones(np.shape(X_train_T), dtype=float)*NDoc_train\n",
    "test_Mat = np.ones(np.shape(X_test_T), dtype=float)*NDoc_train\n",
    "TrainIDF = np.log2(np.divide(train_Mat, DF), dtype=float)\n",
    "TestIDF = np.log2(np.divide(test_Mat, DF), dtype=float)\n",
    "\n",
    "### TF*IDF(use the same idf from training set)\n",
    "TF_IDF_train= np.array(X_train_T * TrainIDF, dtype=float)\n",
    "TF_IDF_test = np.array(X_test_T * TestIDF, dtype=float)\n",
    "\n",
    "TF_IDF_train = TF_IDF_train.T\n",
    "TF_IDF_test = TF_IDF_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 9328) (500, 9328)\n"
     ]
    }
   ],
   "source": [
    "### check the size\n",
    "print(TF_IDF_train.shape, TF_IDF_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Got errors from 2.d: Need to take care of NaNs and infinite values\n",
    "def cleaning(df):\n",
    "    df[np.isnan(df)] = 0\n",
    "    df[df == inf] = 0\n",
    "    df[df == -inf] = 0\n",
    "    return df\n",
    "TF_IDF_train = cleaning(TF_IDF_train)\n",
    "TF_IDF_test = cleaning(TF_IDF_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.d Perform Kmeans clustering on the training data. Write a function to display the top N terms in each cluster along with the cluster DF values for each term, percentage of docs in the cluster in which the terms appear, and the size of the cluster. Sort the terms in decreasing order of the DF percentage. [Extra Credit: use your favorite third party tool, ideally with a Python based API, to create a word cloud for each cluster based on the in-cluster DF values.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copy the K-means scripts here (import doesn't work for python 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Created on Feb 16, 2011\n",
    "k Means Clustering for Ch10 of Machine Learning in Action\n",
    "@author: Peter Harrington\n",
    "'''\n",
    "from numpy import *\n",
    "\n",
    "def distEclud(vecA, vecB):\n",
    "    return sqrt(sum(power(vecA - vecB, 2))) #la.norm(vecA-vecB)\n",
    "\n",
    "def randCent(dataSet, k):\n",
    "    n = shape(dataSet)[1]\n",
    "    centroids = zeros((k,n), dtype=float)\n",
    "    for j in range(n):\n",
    "        minJ = min(dataSet[:,j])\n",
    "        rangeJ = float(max(dataSet[:,j]) - minJ)\n",
    "        centroids[:,j] = minJ + rangeJ * random.rand(k)\n",
    "        \n",
    "    return centroids \n",
    "\n",
    "def kMeans(dataSet, k, distMeas=cos_sim, createCent=randCent):\n",
    "    m = shape(dataSet)[0]\n",
    "    clusterAssment = zeros((m,2))\n",
    "    #to a centroid, also holds SE of each point\n",
    "    centroids = createCent(dataSet, k)\n",
    "    clusterChanged = True\n",
    "    while clusterChanged:\n",
    "        clusterChanged = False\n",
    "        for i in range(m):#for each data point assign it to the closest centroid\n",
    "            minDist = inf; minIndex = -1\n",
    "            for j in range(k):\n",
    "                distJI = distMeas(centroids[j,:],dataSet[i,:])\n",
    "                if distJI < minDist:\n",
    "                    minDist = distJI; minIndex = j\n",
    "            if clusterAssment[i,0] != minIndex: clusterChanged = True\n",
    "            clusterAssment[i,:] = minIndex,minDist**2\n",
    "\n",
    "        for cent in range(k):#recalculate centroids\n",
    "            ptsInClust = dataSet[nonzero(clusterAssment[:,0]==cent)[0]] #get all the point in this cluster - Note: this was incorrect in the original distribution.\n",
    "            if(len(ptsInClust)!=0):\n",
    "                centroids[cent,:] = mean(ptsInClust, axis=0) #assign centroid to mean - Note condition was added 10/28/2013\n",
    "    return centroids, clusterAssment\n",
    "\n",
    "def biKmeans(dataSet, k, distMeas=distEclud):\n",
    "    m = shape(dataSet)[0]\n",
    "    clusterAssment = mat(zeros((m,2)))\n",
    "    centroid0 = mean(dataSet, axis=0).tolist()[0]\n",
    "    centList =[centroid0] #create a list with one centroid\n",
    "    for j in range(m): #calc initial Error\n",
    "        clusterAssment[j,1] = distMeas(mat(centroid0), dataSet[j,:])**2\n",
    "    while (len(centList) < k):\n",
    "        lowestSSE = inf\n",
    "        for i in range(len(centList)):\n",
    "            ptsInCurrCluster = dataSet[nonzero(clusterAssment[:,0].A==i)[0],:] #get the data points currently in cluster i\n",
    "            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas)\n",
    "            sseSplit = sum(splitClustAss[:,1])#compare the SSE to the currrent minimum\n",
    "            sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:,0].A!=i)[0],1])\n",
    "            print(\"sseSplit, and notSplit: \",sseSplit,sseNotSplit)\n",
    "            if (sseSplit + sseNotSplit) < lowestSSE:\n",
    "                bestCentToSplit = i\n",
    "                bestNewCents = centroidMat\n",
    "                bestClustAss = splitClustAss.copy()\n",
    "                lowestSSE = sseSplit + sseNotSplit\n",
    "        bestClustAss[nonzero(bestClustAss[:,0] == 1)[0],0] = len(centList) #change 1 to 3,4, or whatever\n",
    "        bestClustAss[nonzero(bestClustAss[:,0] == 0)[0],0] = bestCentToSplit\n",
    "        print('the bestCentToSplit is: ',bestCentToSplit)\n",
    "        print('the len of bestClustAss is: ', len(bestClustAss))\n",
    "        centList[bestCentToSplit] = bestNewCents[0,:].tolist()[0]#replace a centroid with two best centroids \n",
    "        centList.append(bestNewCents[1,:].tolist()[0])\n",
    "        clusterAssment[nonzero(clusterAssment[:,0].A == bestCentToSplit)[0],:]= bestClustAss#reassign new clusters, and SSE\n",
    "    return mat(centList), clusterAssment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centroids, clusters = kMeans(mat(TF_IDF_train), 5, distMeas=cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Get help from other students for the zip function\n",
    "def top_terms(df, k, N_term):\n",
    "\n",
    "    for i in range(k):\n",
    "        ### First, print out cluster number and the number of obs in it\n",
    "        n_cluster = df[clusters[:,0]==i]\n",
    "        n_obs = n_cluster.shape[0]\n",
    "        print('Cluster: ',i)\n",
    "        print(\"cluster %d has %d observations\" % (i, n_obs))\n",
    "        \n",
    "        ### Calculate document frequency in each cluster        \n",
    "        doc_freq_cluster = np.array([(n_cluster.T!=0).sum(1)]).T\n",
    "        \n",
    "        ### Calculate the percentage of DF\n",
    "        pc_doc_freq_cluster = doc_freq_cluster/float(n_cluster.shape[0])\n",
    "        pc_doc_freq_cluster = map(list, pc_doc_freq_cluster)\n",
    "        pc_doc_freq_cluster = [i[0] for i in pc_doc_freq_cluster]\n",
    "        cluster_DF = [i[0] for i in doc_freq_cluster]\n",
    "        \n",
    "        ### Zip them together\n",
    "        sort=sorted(zip(terms, doc_freq_cluster, pc_doc_freq_cluster),key=lambda x:x[2],reverse=True)\n",
    "        for i in sort[:N_term]:\n",
    "               print(i[0], '\\t', i[1], '\\t', i[2])\n",
    "        print('\\n')\n",
    "       \n",
    "    return centroids, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 0 :\n",
      "cluster 0  size:  402\n",
      "term \t frequency \t PercentOfDocs\n",
      "['subject'] \t [402] \t 1.0\n",
      "['write'] \t [247] \t 0.614427860697\n",
      "['god'] \t [223] \t 0.554726368159\n",
      "['on'] \t [220] \t 0.547263681592\n",
      "['christian'] \t [196] \t 0.487562189055\n",
      "\n",
      "\n",
      "cluster 1 :\n",
      "cluster 1  size:  394\n",
      "term \t frequency \t PercentOfDocs\n",
      "['subject'] \t [394] \t 1.0\n",
      "['write'] \t [221] \t 0.560913705584\n",
      "['game'] \t [210] \t 0.532994923858\n",
      "['team'] \t [183] \t 0.464467005076\n",
      "['go'] \t [176] \t 0.446700507614\n",
      "\n",
      "\n",
      "cluster 2 :\n",
      "cluster 2  size:  395\n",
      "term \t frequency \t PercentOfDocs\n",
      "['subject'] \t [395] \t 1.0\n",
      "['window'] \t [278] \t 0.703797468354\n",
      "['write'] \t [176] \t 0.445569620253\n",
      "['file'] \t [149] \t 0.377215189873\n",
      "['get'] \t [139] \t 0.351898734177\n",
      "\n",
      "\n",
      "cluster 3 :\n",
      "cluster 3  size:  424\n",
      "term \t frequency \t PercentOfDocs\n",
      "['subject'] \t [424] \t 1.0\n",
      "['sale'] \t [237] \t 0.558962264151\n",
      "['email'] \t [165] \t 0.389150943396\n",
      "['pleas'] \t [154] \t 0.36320754717\n",
      "['offer'] \t [116] \t 0.27358490566\n",
      "\n",
      "\n",
      "cluster 4 :\n",
      "cluster 4  size:  385\n",
      "term \t frequency \t PercentOfDocs\n",
      "['subject'] \t [385] \t 1.0\n",
      "['write'] \t [265] \t 0.688311688312\n",
      "['articl'] \t [202] \t 0.524675324675\n",
      "['clipper'] \t [200] \t 0.519480519481\n",
      "['kei'] \t [188] \t 0.488311688312\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "centroids, clusters = top_terms(np.array(TF_IDF_train), 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.e Using the cluster assignments from Kmeans clustering, compare your 5 clusters to the 5 pre-assigned classes by computing the Completeness and Homogeneity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.,  1.,  3., ...,  3.,  1.,  3.])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness is 0.8\n",
      "Homogeneity is 0.8\n"
     ]
    }
   ],
   "source": [
    "com = completeness_score(np.array(y_train.T)[0],cluster[:,0])\n",
    "hom = homogeneity_score(np.array(y_train.T)[0],cluster[:,0])\n",
    "print (\"Completeness is %0.001f\" % (com))\n",
    "print (\"Homogeneity is %0.001f\" % (hom))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
